---
layout: post
title: "《神经网络与深度学习》第一章学习笔记"
date: 2020-05-18 
description: "深度学习"
tag: 深度学习
---



### 《神经网络与深度学习》第一章学习笔记

深度学习是`机器学习`的一个分支，是指一类`问题`以及解决这类问题的`方法`.

首先，深度学习问题是一个机器学习问题，指从有限样例中通过算法总结出一般性的规律，并可以应用到新的未知数据上.

其次，深度学习采用的模型一般比较复杂，指样本的原始输入到输出目标之间的数据流经过多个线性或非线性的组件. 因为每个组件都会对信息进行加工，并进而影响后续的组件，所以当我们最后得到输出结果时，我们并不清楚其中每个组件的贡献是多少.  这个问题叫做贡献度分配问题（Credit Assignment Problem，CAP）`人工神经网络`（Artificial Neural Network，ANN)可以较好解决贡献度分配的问题.

神经网络一般比较复杂，从输入到输出的信息传递路径一般比较长，所以复杂神将网络的学习可以看成是一种深度的机器学习，即深度学习.

`神经网络和深度学习并不等价`.深度学习可以采用神经网络模型，也可以采用其他模型（比如深度信念网络是一种概率图模型）. 但是由于神经网络模型可以比较容易地解决贡献度分配问题，因此神经网络模型成为深度学习中主要采用的模型.

**1.1 人工智能**

简单来说，人工智能（artificial intelligence，AI）就是让机器具有人类的智慧，这也是人们长期追求的目标. 这里关于“智能”并没有一个明确的定义.

1950 年，阿兰·图灵（Alan Turing）发表了一篇有着重要影响力的论文《Computing Machinery and Intelligence》，讨论了创造一种“智能机器”的可能性. 由于“智能”一词比较难以定义，他提出了著名的图灵测试：“一个人在不接触对方的情况下，通过一种特殊的方式和对方进行一系列的问答. 如果在相当长时间内，他无法根据这些问题判断对方是人还是计算机，那么就可以认为这个计算机是智能的”.

和很多其他学科不同，人工智能这个学科的诞生有着明确的标志性事件，就是1956 年的达特茅斯（Dartmouth）会议.在这次会议上，“人工智能”被提出并作为本研究领域的名称. 同时，人工智能研究的使命也得以确定. John McCarthy （约翰·麦卡锡）提出了人工智能的定义：人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样.

目前，人工智能的主要领域大体上可以分为以下几个方面：

（1） `感知`：模拟人的感知能力，对外部刺激信息（视觉和语音等）进行感知和加工. 主要研究领域包括语音信息处理和计算机视觉等.

（2） `学习`：模拟人的学习能力，主要研究如何从样例或从与环境的交互中进行学习. 主要研究领域包括监督学习、无监督学习和强化学习等.

（3） `认知`：模拟人的认知能力，主要研究领域包括知识表示、自然语言理解、推理、规划、决策等.

***1.1.1 人工智能的发展历史***

人工智能从诞生至今，经历了一次又一次的繁荣与低谷，其发展历程大体上可以分为“推理期”、“知识期”和“学习期”

（1）推理期

1956 年达特茅斯会议之后，研究者对人工智能的热情高涨，之后的十几年是人工智能的黄金时期. 大部分早期研究者都通过人类的经验，基于逻辑或者事实归纳出来一些规则，然后通过编写程序来让计算机完成一个任务. 这个时期中，研究者开发了一系列的智能系统，比如几何定理证明器、语言翻译器等.

（2）知识期

到了20世纪70年代，研究者意识到`知识`对于人工智能系统的重要性. 在这一时期，出现了各种各样的专家系统（Expert System），并在特定的领域取得了很多成果. 专家系统可以简单理解为“知识库+推理机”，是一类具有专门知识和经验的计算机智能系统.专家系统也被称为`基于知识的系统`.专家系统的三要素：1）领域专家知识；2）模拟专家思维；3）达到专家级的水平

（3）学习期

对于人类的很多智能行为（比如语言理解、图像理解等），很难知道其中的原理，也无法描述这些智能行为背后的“知识”. 因此，很难通过知识和推理的方式来实现这些行为的智能系统. 为了解决这类问题，研究者开始将研究重点转向让计算机从数据中自己学习. 事实上，“学习”本身也是一种智能行为. 从人工智能的萌芽时期开始，就有一些研究者尝试让机器来自动学习，即`机器学习`（Machine Learning，ML）. 机器学习的主要目的是设计和分析一些`学习算法`，让计算机可以从数据（经验）中自动分析并获得规律，之后利用学习到的规律对未知数据进行预测，从而帮助人们完成一些特定任务，提高开发效率.机器学习的研究内容也十分广泛，涉及线性代数、概率论、统计学、数学优化、计算复杂性等多门学科．在人工智能领域，机器学习从一开始就是一个重要的研究方向. 但直到1980 年后，机器学习因其在很多领域的出色表现，才逐渐成为热门学科．

下图给出了人工智能发展历史上的重要事件.

![](/images/posts/ai1/pic1.png)

***1.1.2 人工智能的流派***

尽管人工智能的流派非常多，但主流的方法大体上可以归结为以下两种：

（1） 符号主义（Symbolism），又称逻辑主义、心理学派或计算机学派，是指通过分析人类智能的功能，然后用计算机来实现这些功能的一类方法.符号主义有两个基本假设：a）信息可以用符号来表示；b）符号可以通过显式的规则（比如逻辑运算）来操作.人类的认知过程可以看作符号操作过程. 在人工智能的推理期和知识期，符号主义的方法比较盛行，并取得了大量的成果.

（2） 连接主义（Connectionism），又称仿生学派或生理学派，是认知科学领域中的一类信息处理的方法和理论.在认知科学领域，人类的认知过程可以看作一种信息处理过程. 连接主义认为人类的认知过程是由大量简单神经元构成的神经网络中的信息处理过程，而不是符号运算. 因此，连接主义模型的主要结构是由大量简单的信息处理单元组成的互联网络，具有非线性、分布式、并行化、局部性计算以及自适应性等特性.

符号主义方法的一个优点是可解释性，而这也正是连接主义方法的弊端.深度学习的主要模型神经网络就是一种连接主义模型. 随着深度学习的发展，越来越多的研究者开始关注如何融合符号主义和连接主义，建立一种高效并且具有可解释性的模型.

**1.2 机器学习**

`机器学习`（machine learning，ML）是指从有限的观测数据中学习（或“猜测”）出具有一般性的规律，并利用这些规律对未知数据进行预测的方法. 机器学习是人工智能的一个重要分支，并逐渐成为推动人工智能发展的关键因素. 

![](/images/posts/ai1/pic2.png)

传统的机器学习主要关注如何学习一个`预测模型`．一般需要首先将数据表示为一组`特征`（Feature），特征的表示形式可以是连续的数值、离散的符号或其他形式．然后将这些特征输入到预测模型，并输出预测结果．这类机器学习可以看作`浅层学习`（Shallow Learning）．浅层学习的一个重要特点是不涉及特征学习，其特征主要靠人工经验或特征转换方法来抽取．特征的提取依靠经验，且是一个费时费力的过程.

在实际任务中使用机器学习模型一般会包含以下几个步骤：

（1） `数据预处理`：经过数据的预处理，如去除噪声等．比如在文本分类中，去除停用词等．

（2） `特征提取`：从原始数据中提取一些有效的特征．比如在图像分类中，提取边缘、尺度不变特征变换（Scale Invariant Feature Transform，SIFT）特征等．

（3） `特征转换`：对特征进行一定的加工，比如降维和升维. 很多特征转换方法也都是机器学习方法．降维包括`特征抽取`（Feature Extraction）和`特征选择`（Feature Selection）两种途径. 常用的特征转换方法有主成分分析（Principal Components Analysis，PCA）、线性判别分析（Linear Discriminant Analysis，LDA）等．

（4） `预测`：机器学习的核心部分，学习一个函数并进行预测．

![](/images/posts/ai1/pic3.png)

上述流程中，每步特征处理以及预测一般都是分开进行的. 传统的机器学习模型主要关注最后一步，即构建预测函数. 但是实际操作过程中，不同预测模型的性能相差不多，而前三步中的特征处理对最终系统的准确性有着十分关键的作用. 特征处理一般都需要人工干预完成，利用人类的经验来选取好的特征，并最终提高机器学习系统的性能. 因此，很多的机器学习问题变成了`特征工程`（Feature Engineering）问题. 开发一个机器学习系统的主要工作量都消耗在了预处理、特征提取以及特征转换上．

**1.3 表示学习**

为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的`特征`，或者更一般性地称为表示（Representation）. 如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作`表示学习`（Representation Learning）．我个人理解，表示学习中一个比较典型的方法就是自编码器（Auto-Encoder）.

表示学习的关键是解决`语义鸿沟`（Semantic Gap）问题. 语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性. 比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，不同图片在像素级别上的表示（即底层特征）差异性也会非常大. 但是我们人理解这些图片是建立在比较抽象的高层语义概念上的. 如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高. 如果可以有一个好的表示在某种程度上可以反映出数据的高层语义特征，那么我们就可以相对容易地构建后续的机器学习模型.

在表示学习中，有两个核心问题：一是“什么是一个好的表示？”；二是“如何学习到好的表示？”

好的表示没有明确的标准，但通常具有以下几个优点：

- 一个好的表示应该具有`很强的表示能力`，即同样大小的向量可以表示更多信息.
- 一个好的表示应该`使后续的学习任务变得简单`，即需要包含更高层的语义信息.
- 一个好的表示应该`具有一般性，是任务或领域独立的`. 虽然目前的大部分表示学习方法还基于某个任务来学习，但我们期望学到的表示可以比较容易迁移到其它任务上.

在传统机器学习中，我们经常使用两种方式来表示特征：`局部表示`（Local Representation）和`分布式表示`（Distributed Representation）.

以颜色表示为例，一种表示颜色的方法是以不同名字来命名不同的颜色，这种表示方式叫做局部表示，也称为离散表示或符号表示. 局部表示通常可以表示为`one-hot`向量的形式. 假设有$v$种颜色，我们可以用一个大小为$1 \times v$的one-hot向量表示每种颜色. 在第$i$种颜色对应的one-hot向量中，第$i$维的值为1，其余为0.

局部表示有两个优点：1）这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程；2）通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高．

局部表示有两个不足之处：1）one-hot 向量的**维数很高，且不能扩展**。如果有一种新的颜色，我们就需要增加一维来表示；2）不同颜色之间的**相似度**都为0，即我们无法知道“红色”和“中国红”的相似度要比“红色”和“黑色”的相似度要高.

另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应R、G、B三维空间中一个点，这种表示方式叫做分布式表示. **分布式表示通常可以表示为低维的稠密向量**.

和局部表示相比，分布式表示的表示能力要比局部表示强很多，分布式表示的向量维度一般都比较低。我们只需要用一个三维的稠密向量就可以表示所有颜色. 并且分布式表示也很容易表示新的颜色名. 此外，不同颜色之间的相似度也很容易计算。

![](/images/posts/ai1/pic4.png)

我们可以使用神经网络来将高维的局部表示空间$R^{\nu}$映射到一个非常低维的分布式表示空间$R^d$ , 其中$d \ll \nu$. 在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中. 我个人理解，最典型的例子还是自编码器（Auto-Encoder）.

在机器学习中，这个过程也称为**嵌入（Embedding）**. **嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系**. 比如自然语言中词的分布式表示，也经常叫做词嵌入.

如下图所示，将一个3维的one-hot向量和一个2维嵌入空间的对比. 在one-hot向量空间中，每个特征都位于坐标轴上，每个坐标轴上一个特征.  而在低维的嵌入空间中，每个特征都不在坐标轴上，特征之间可以计算相似度。

![](/images/posts/ai1/pic5.png)

**1.4 深度学习**

为了学习一种好的表示，需要构建具有一定“深度”的模型，并通过学习算法来让模型自动学习出好的特征表示（从底层特征，到中层特征，再到高层特征），从而最终提升预测模型的准确率．所谓“深度”是指原始数据进行非线性特征转换的次数．如果把一个表示学习系统看作一个有向图结构，深度也可以看作从输入节点到输出节点所经过的最长路径的长度．

这样我们就需要一种学习方法可以从数据中学习一个“深度模型”，这就是深度学习（Deep Learning，DL）．深度学习是机器学习的一个子问题，其主要目的是从数据中自动学习到有效的特征表示，从而可以替代人工设计的特征，避免“特征工程”.

![](/images/posts/ai1/pic6.png)

在一些复杂任务中，传统机器学习方法需要将一个任务的输入和输出之间人为地切割成`很多子模块`（或多个阶段），每个子模块分开学习. 比如一个自然语言理解任务，一般需要分词、词性标注、句法分析、语义分析、语义推理等步骤. 这种学习方式有两个问题：一是每一个模块都需要单独优化，并且其优化目标和任务总体目标并不能保证一致；二是错误传播，即前一步的错误会对后续的模型造成很大的影响. 这样就增加了机器学习方法在实际应用中的难度. `端到端学习`（End-to-End Learning），也称`端到端训练`，是指在学习过程中不进行分模块或分阶段训练，直接优化任务的总体目标. 在端到端学习中，一般不需要明确地给出不同模块或阶段的功能，中间过程不需要人为干预. 端到端学习的训练数据为“输入-输出”对的形式，无须提供其他额外信息. 因此，端到端学习和深度学习一样，都是要解决贡献度分配问题. 目前，大部分采用神经网络模型的深度学习也可以看作一种端到端的学习. 

**1.5 神经网络**

***1.5.1 人工神经网络***

人工神经网络（Artificial Neural Network，ANN）是为模拟人脑神经网络而设计的一种计算模型，它从结构、实现机理和功能上模拟人脑神经网络. 人工神经网络与生物神经元类似，由多个节点（人工神经元）互相连接而成，可以用来对数据之间的复杂关系进行建模. 不同节点之间的连接被赋予了不同的权重，每个权重代表了一个节点对另一个节点的影响大小. 每个节点代表一种特定函数，来自其他节点的信息经过其相应的权重综合计算，输入到一个激活函数中并得到一个新的活性值（兴奋或抑制）. 从系统观点看，人工神经元网络是由大量神经元通过极其丰富和完善的连接而构成的`自适应非线性动态系统`．

虽然我们可以比较容易地构造一个人工神经网络，但是如何让人工神经网络具有学习能力并不是一件容易的事情. 早期的神经网络模型并不具备学习能力. 首个可学习的人工神经网络是赫布网络，采用一种基于赫布规则的无监督学习方法. 感知器是最早的具有机器学习思想的神经网络，但其学习方法无法扩展到多层的神经网络上. 直到1980年左右，反向传播算法才有效地解决了多层神经网络的学习问题，并成为最为流行的神经网络学习算法. 

人工神经网络主要是作为一种映射函数，即机器学习中的模型．由于人工神经网络可以用作一个通用的函数逼近器（**一个两层的神经网络可以逼近任意的函数**），因此我们可以将人工神经网络看作一个可学习的函数，并将其应用到机器学习中．**理论上，只要有足够的训练数据和神经元数量，人工神经网络就可以学到很多复杂的函数**．我们可以把一个人工神经网络塑造复杂函数的能力称为网络容量（Network Capacity），这与可以被储存在网络中的信息的复杂度以及数量相关．

***1.5.2 神经网络的发展***

`第一阶段：模型提出`  第一阶段为1943 年～1969 年，是神经网络发展的第一个高潮期．在此期间，科学家们提出了许多神经元模型和学习规则．

1943 年，心理学家Warren McCulloch 和数学家Walter Pitts 最早提出了一种基于简单逻辑运算的人工神经网络，这种神经网络模型称为`MP 模型`，至此开启了人工神经网络研究的序幕．1948年，Alan Turing 提出了一种“B 型图灵机”．“B 型图灵机”可以基于Hebbian 法则来进行学习．1951年，McCulloch和Pitts 的学生Marvin Minsky 建造了第一台神经网络机SNARC．提出了一种可以模拟人类感知能力的神经网络模型，称为`感知器`（Perceptron），并提出了一种接近于人类学习过程（迭代、试错）的学习算法．

在这一时期，神经网络以其独特的结构和处理信息的方法，在许多实际应用领域（自动控制、模式识别等）中取得了显著的成效．

`第二阶段：冰河期`  第二阶段为1969 年～1983 年，是神经网络发展的第一个低谷期．在此期间，神经网络的研究处于长年停滞及低潮状态．

1969 年，Marvin Minsky 出版《感知器》一书，指出了神经网络的两个关键缺陷：一是感知器无法处理“异或”回路问题；二是当时的计算机无法支持处理大型神经网络所需要的计算能力．这些论断使得人们对以感知器为代表的神经网络产生质疑，并导致神经网络的研究进入了十多年的“冰河期”．

但在这一时期，依然有不少学者提出了很多有用的模型或算法．1974 年，哈佛大学的Paul Werbos 发明`反向传播算法`（BackPropagation，BP）[Werbos,1974]，但当时未受到应有的重视．1980 年，福岛邦彦提出了一种带卷积和子采样操作的多层神经网络：`新知机`（Neocognitron）[Fukushima, 1980]．新知机的提出是受到了动物初级视皮层简单细胞和复杂细胞的感受野的启发．但新知机并没有采用反向传播算法，而是采用了无监督学习的方式来训练，因此也没有引起足够的重视．

`第三阶段：反向传播算法引起的复兴`  第三阶段为1983 年～1995 年，是神经网络发展的第二个高潮期．这个时期中，反向传播算法重新激发了人们对神经网络的兴趣．

1983 年，物理学家John Hopfield 提出了一种用于联想记忆（Associative Memory）的神经网络，称为Hopfield 网络．Hopfield 网络在旅行商问题上取得了当时最好结果，并引起了轰动． 1984 年，Geoffrey Hinton 提出一种随机化版本的Hopfield 网络，即玻尔兹曼机（Boltzmann Machine）．

真正引起神经网络第二次研究高潮的是反向传播算法．20世纪80年代中期，一种连接主义模型开始流行，即分布式并行处理（Parallel Distributed Processing，PDP）模型[McClelland et al., 1986]．反向传播算法也逐渐成为PDP 模型的主要学习算法．这时，神经网络才又开始引起人们的注意，并重新成为新的研究热点．随后，[LeCun et al., 1989] 将反向传播算法引入了卷积神经网络，并在手写体数字识别上取得了很大的成功[LeCun et al., 1998]．反向传播算法是迄今最为成功的神经网络学习算法．目前在深度学习中主要使用的自动微分可以看作反向传播算法的一种扩展．

然而，梯度消失问题（Vanishing Gradient Problem）阻碍神经网络的进一步发展，特别是循环神经网络．为了解决这个问题，[Schmidhuber, 1992] 采用两步来训练一个多层的循环神经网络：1）通过无监督学习的方式来逐层训练每一层循环神经网络，即预测下一个输入；2）通过反向传播算法进行精调．

`第四阶段：流行度降低`  第四阶段为1995 年～2006 年，在此期间，支持向量机和其他更简单的方法（例如线性分类器）在机器学习领域的流行度逐渐超过了神经网络．

虽然神经网络可以很容易地增加层数、神经元数量，从而构建复杂的网络，但其计算复杂性也会随之增长．当时的计算机性能和数据规模不足以支持训练大规模神经网络．在20 世纪90 年代中期，统计学习理论和以支持向量机为代表的机器学习模型开始兴起．相比之下，神经网络的理论基础不清晰、优化困难、可解释性差等缺点更加凸显，因此神经网络的研究又一次陷入低潮．

`第五阶段：深度学习的崛起`  第五阶段为从2006 年开始至今，在这一时期研究者逐渐掌握了训练深层神经网络的方法，使得神经网络重新崛起．

[Hinton et al., 2006] 通过逐层预训练来学习一个深度信念网络， 深度信念网络参见并将其权重作为一个多层前馈神经网络的初始化权重，再用反向传播算法进行精调．这种“预训练+ 精调”的方式可以有效地解决深度神经网络难以训练的问题．随着深度神经网络在语音识别[Hinton et al., 2012] 和图像分类[Krizhevsky et al.,2012] 等任务上的巨大成功，以神经网络为基础的深度学习迅速崛起．近年来，随着大规模并行计算以及GPU 设备的普及，计算机的计算能力得以大幅提高．此外，可供机器学习的数据规模也越来越大．在强大的计算能力和海量的数据规模
支持下，计算机已经可以端到端地训练一个大规模神经网络，不再需要借助预训练的方式．各大科技公司都投入巨资研究深度学习，神经网络迎来第三次高潮．